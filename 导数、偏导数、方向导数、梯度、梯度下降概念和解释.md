非原创,转自[csdn的一篇文章](https://blog.csdn.net/weixin_39198406/article/details/100151260)
## 前言  
机器学习中的大部分问题都是优化问题，而绝大部分优化问题都可以使用梯度下降法处理，那么搞懂什么是梯度，什么是梯度下降法就非常重要！这是基础中的基础，也是必须掌握的概念！

提到梯度，就必须从导数（derivative）、偏导数（partial derivative）和方向导数（directional derivative）讲起，弄清楚这些概念，才能够正确理解为什么在优化问题中使用梯度下降法来优化目标函数，并熟练掌握梯度下降法(Gradient Descent).

1. 导数
2. 导数和偏导数
3. 导数与方向导数
4. 导数与梯度
5. 梯度下降法

## 导数  
　一张图读懂导数与微分： 

![image](https://user-images.githubusercontent.com/13010679/113124086-bf045e80-9247-11eb-987f-c207f9861727.png)

这是高数中的一张经典图，如果忘记了导数微分的概念，基本看着这张图就能全部想起来.

导数定义如下： 

 ![image](https://user-images.githubusercontent.com/13010679/113124141-ca578a00-9247-11eb-9b4c-34cb82479ffe.png)

反映的是函数y=f(x)在某一点处沿x轴正方向的变化率。再强调一遍，是函数f(x)在x轴上某一点处沿着x轴正方向的变化率/变化趋势。直观地看，也就是在x轴上某一点处，如果f’(x)>0，说明f(x)的函数值在x点沿x轴正方向是趋于增加的；如果f’(x)<0，说明f(x)的函数值在x点沿x轴正方向是趋于减少的。

这里补充上图中的Δy、dy等符号的意义及关系如下： 
　Δx：x的变化量；  
　dx：x的变化量Δx趋于0时，则记作微元dx；  
　Δy：Δy=f(x0+Δx)-f(x0)，是函数的增量；  
　dy：dy=f’(x0)dx，是切线的增量；  
　当Δx→0时，dy与Δy都是无穷小，dy是Δy的主部，即Δy=dy+o(Δx).  

## 导数和偏导数  
　偏导数的定义如下：  

![image](https://user-images.githubusercontent.com/13010679/113124293-f4a94780-9247-11eb-9ab8-382bbbc7a71f.png)

可以看到，导数与偏导数本质是一致的，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。
　区别在于：
　导数，指的是一元函数中，函数y=f(x)在某一点处沿x轴正方向的变化率；
　偏导数，指的是多元函数中，函数y=f(x1,x2,…,xn)在某一点处沿某一坐标轴（x1,x2,…,xn）正方向的变化率。

## 导数与方向导数
　方向导数的定义如下：

![image](https://user-images.githubusercontent.com/13010679/113124462-1c98ab00-9248-11eb-8b88-a723c8625e5b.png)

在前面导 数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当我们讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。
　通俗的解释是：
　我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。

## 导数与梯度
　梯度的定义如下：

![image](https://user-images.githubusercontent.com/13010679/113124548-32a66b80-9248-11eb-8fac-1bfbabf1cbab.png)

梯度的提出只为回答一个问题：
　函数在变量空间的某一点处，沿着哪一个方向有最大的变化率？
　梯度定义如下：
　函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。
　这里注意三点：
　1）梯度是一个向量，即有方向有大小；
　2）梯度的方向是最大方向导数的方向；
　3）梯度的值是最大方向导数的值。

## 导数与向量
提问：导数与偏导数与方向导数是向量么？
向量的定义是有方向（direction）有大小（magnitude）的量。
从前面的定义可以这样看出，偏导数和方向导数表达的是函数在某一点沿某一方向的变化率，也是具有方向和大小的。因此从这个角度来理解，我们也可以把偏导数和方向导数看作是一个向量，向量的方向就是变化率的方向，向量的模，就是变化率的大小。
么沿着这样一种思路，就可以如下理解梯度：
梯度即函数在某一点最大的方向导数，函数沿梯度方向函数有最大的变化率。

## 梯度下降法
　既然在变量空间的某一点处，函数沿梯度方向具有最大的变化率，那么在优化目标函数的时候，自然是沿着负梯度方向去减小函数值，以此达到我们的优化目标。
　如何沿着负梯度方向减小函数值呢？既然梯度是偏导数的集合，如下：

![image](https://user-images.githubusercontent.com/13010679/113124586-3df99700-9248-11eb-9401-a74ff5b8edfb.png)

同时梯度和偏导数都是向量，那么参考向量运算法则，我们在每个变量轴上减小对应变量值即可，梯度下降法可以描述如下：

![image](https://user-images.githubusercontent.com/13010679/113124620-48b42c00-9248-11eb-8fce-03f4fa6abaf2.png)

**以上就是梯度下降法的由来，大部分的机器学习任务，都可以利用Gradient Descent来进行优化。**

## 总结：

1.导数定义： 导数代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量的变化的比值。几何意义是这个点的切线。物理意义是该时刻的（瞬时）变化率。

注意：在一元函数中，只有一个自变量变动，也就是说只存在一个方向的变化率，这也就是为什么一元函数没有偏导数的原因。
（derivative）

2.偏导数： 既然谈到偏导数，那就至少涉及到两个自变量。以两个自变量为例，z=f（x,y），从导数到偏导数，也就是从曲线来到了曲面。曲线上的一点，其切线只有一条。但是曲面上的一点，切线有无数条。而偏导数就是指多元函数沿着坐标轴的变化率。
注意：直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。
（partial derivative）

3.方向导数: 在某点沿着某个向量方向上的方向导数，描绘了该点附近沿着该向量方向变动时的瞬时变化率。这个向量方向可以是任一方向。

方向导数的物理意义表示函数在某点沿着某一特定方向上的变化率。
注意：导数、偏导数和方向导数表达的是函数在某一点沿某一方向的变化率，也是具有方向和大小的。
（directional derivative）

4.梯度: 函数在给定点处沿不同的方向，其方向导数一般是不相同的。那么沿着哪一个方向其方向导数最大，其最大值为多少，这是我们所关心的，为此引进一个很重要的概念: 梯度。

5.梯度下降
在机器学习中往往是最小化一个目标函数 L(Θ)，理解了上面的内容，便很容易理解在梯度下降法中常见的参数更新公式：

Θ = Θ − γ ∂ L ∂ Θ
通过算出目标函数的梯度（算出对于所有参数的偏导数）并在其反方向更新完参数 Θ ，在此过程完成后也便是达到了函数值减少最快的效果，那么在经过迭代以后目标函数即可很快地到达一个极小值。

6.In summary:
|概念 　　|物理意义|
|--|--|
|导数  |　　函数在该点的瞬时变化率|
|偏导数 |　函数在坐标轴方向上的变化率|
|方向导数 | 函数在某点沿某个特定方向的变化率|
|梯度  |　　函数在该点沿所有方向变化率最大的那个方向|

